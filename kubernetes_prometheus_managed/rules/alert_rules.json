[
  {
    "alert": "KubeCPUQuotaOvercommit",
    "enabled": true,
    "expression": "sum(min without(resource) (kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\", resource=~\"(cpu|requests.cpu)\"}))  /sum(kube_node_status_allocatable{resource=\"cpu\", job=\"kube-state-metrics\"})  > 1.5",
    "severity": 4,
    "for": "PT5M",
    "severity_label": "info",
    "annotations": {
      "summary": "Cluster CPU Quota Overcommitted",
      "description": "Cluster {{ $labels.cluster}} has overcommitted CPU resource requests for Namespaces. For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/cluster-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeMemoryQuotaOvercommit",
    "enabled": true,
    "expression": "sum(min without(resource) (kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\", resource=~\"(memory|requests.memory)\"}))  /sum(kube_node_status_allocatable{resource=\"memory\", job=\"kube-state-metrics\"})  > 1.5",
    "severity": 4,
    "for": "PT5M",
    "severity_label": "info",
    "annotations": {
      "summary": "Cluster MEMORY Quota Overcommitted",
      "description": "Cluster {{ $labels.cluster}} has overcommitted memory resource requests for Namespaces. For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/cluster-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeContainerOOMKilledCount",
    "enabled": true,
    "expression": "sum by (cluster,container,controller,namespace)(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"} * on(cluster,namespace,pod) group_left(controller) label_replace(kube_pod_owner, \"controller\", \"$1\", \"owner_name\", \"(.*)\")) > 0",
    "severity": 3,
    "for": "PT5M",
    "severity_label": "info",
    "annotations": {
      "summary": "Number of OOM killed containers is greater than 0",
      "description": "Number of OOM killed containers is greater than 0. For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/cluster-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeClientErrors",
    "enabled": true,
    "expression": "(sum(rate(rest_client_requests_total{code=~\"5..\"}[5m])) by (cluster, instance, job, namespace)  / sum(rate(rest_client_requests_total[5m])) by (cluster, instance, job, namespace)) > 0.01",
    "severity": 1,
    "severity_label": "critical",
    "for": "PT15M",
    "annotations": {
      "summary": "Kubernetes API server client",
      "description": "Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors. For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/cluster-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubePersistentVolumeFillingUp",
    "enabled": true,
    "expression": "kubelet_volume_stats_available_bytes{job=\"kubelet\"}/kubelet_volume_stats_capacity_bytes{job=\"kubelet\"} < 0.15 and kubelet_volume_stats_used_bytes{job=\"kubelet\"} > 0 and predict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\"}[6h], 4 * 24 * 3600) < 0 unless on(namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{ access_mode=\"ReadOnlyMany\"} == 1 unless on(namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT60M",
    "annotations": {
      "summary": "PersistentVolume claimed is expected to fill up within 4 days",
      "description": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available. For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/cluster-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT15M"
    }
  },
  {
    "alert": "KubePersistentVolumeInodesFillingUp",
    "enabled": true,
    "expression": "kubelet_volume_stats_inodes_free{job=\"kubelet\"} / kubelet_volume_stats_inodes{job=\"kubelet\"} < 0.03",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT15M",
    "annotations": {
      "summary": "PersistentVolume claimed inodes threshold",
      "description": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} only has {{ $value | humanizePercentage }} free inodes. For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/cluster-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubePersistentVolumeErrors",
    "enabled": true,
    "expression": "kube_persistentvolume_status_phase{phase=~\"Failed|Pending\",job=\"kube-state-metrics\"} > 0",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT5M",
    "annotations": {
      "summary": "The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/cluster-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeContainerWaiting",
    "enabled": true,
    "expression": "sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{job=\"kube-state-metrics\"}) > 0",
    "severity": 1,
    "severity_label": "error",
    "for": "PT60M",
    "annotations": {
      "summary": "Container waiting state for longer than 1 hour.",
      "description": "pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour. For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/cluster-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeDaemonSetNotScheduled",
    "enabled": true,
    "expression": "kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\"} - kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\"} > 0",
    "severity": 3,
    "severity_label": "info",
    "for": "PT15M",
    "annotations": {
      "summary": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/cluster-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeDaemonSetMisScheduled",
    "enabled": true,
    "expression": "kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\"} > 0",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT15M",
    "annotations": {
      "summary": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/cluster-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeQuotaAlmostFull",
    "enabled": true,
    "expression": "kube_resourcequota{job=\"kube-state-metrics\", type=\"used\"}  / ignoring(instance, job, type)(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\"} > 0)  > 0.9 < 1",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT15M",
    "annotations": {
      "summary": "{{ $value | humanizePercentage }} usage of {{ $labels.resource }} in namespace {{ $labels.namespace }} in {{ $labels.cluster}}.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/cluster-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubePersistentVolumeClaimNotBound",
    "enabled": true,
    "expression": "kube_persistentvolumeclaim_status_condition{condition='Bound',status='false'} == 1",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT5M",
    "annotations": {
      "summary": "PersistentVolumeClaim {{ $labels.pvc }} in namespace {{ $labels.namespace }} is not bound.",
      "description": "Please check the status of the PVC and the associated PV."
    },
    "alert_resolution": {
      "auto_resolved": false,
      "time_to_resolve": "PT15M"
    }
  },
  {
    "alert": "KubeNodeUnreachable",
    "enabled": true,
    "expression": "(kube_node_spec_taint{job=\"kube-state-metrics\",key=\"node.kubernetes.io/unreachable\",effect=\"NoSchedule\"} unless ignoring(key,value) kube_node_spec_taint{job=\"kube-state-metrics\",key=~\"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn\"}) == 1",
    "severity": 1,
    "severity_label": "error",
    "for": "PT15M",
    "annotations": {
      "summary": "{{ $labels.node }} in {{ $labels.cluster}} is unreachable and some workloads may be rescheduled.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/node-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeNodeReadinessFlapping",
    "enabled": true,
    "expression": "sum(changes(kube_node_status_condition{status=\"true\",condition=\"Ready\"}[15m])) by (cluster, node) > 2",
    "severity": 1,
    "severity_label": "error",
    "for": "PT15M",
    "annotations": {
      "summary": "Node {{ $labels.node }} readiness status change multiple times.",
      "description": "The readiness status of node {{ $labels.node }} in {{ $labels.cluster}} has changed more than 2 times in the last 15 minutes. For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/node-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubePVUsageHigh",
    "enabled": true,
    "expression": "avg by (namespace, controller, container, cluster)(((kubelet_volume_stats_used_bytes{job=\"kubelet\"} / on(namespace,cluster,pod,container) group_left kubelet_volume_stats_capacity_bytes{job=\"kubelet\"}) * on(namespace, pod, cluster) group_left(controller) label_replace(kube_pod_owner, \"controller\", \"$1\", \"owner_name\", \"(.*)\")) > .8)",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT15M",
    "annotations": {
      "summary": "Average PV usage on pod {{ $labels.pod }} is over 80%.",
      "description": "Average PV usage on pod {{ $labels.pod }} in container {{ $labels.container }}  is greater than 80%. For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeDeploymentReplicasMismatch",
    "enabled": true,
    "expression": "(  kube_deployment_spec_replicas{job=\"kube-state-metrics\"}    >  kube_deployment_status_replicas_available{job=\"kube-state-metrics\"}) and (  changes(kube_deployment_status_replicas_updated{job=\"kube-state-metrics\"}[10m])    ==  0)",
    "severity": 1,
    "severity_label": "error",
    "for": "PT15M",
    "annotations": {
      "summary": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} in {{ $labels.cluster}} replica mismatch.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT15M"
    }
  },
  {
    "alert": "KubeStatefulSetReplicasMismatch",
    "enabled": true,
    "expression": "(  kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\"}    !=  kube_statefulset_status_replicas{job=\"kube-state-metrics\"}) and (  changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\"}[10m])    ==  0)",
    "severity": 1,
    "severity_label": "error",
    "for": "PT15M",
    "annotations": {
      "summary": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} in {{ $labels.cluster}} replica mismatch.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeHpaReplicasMismatch",
    "enabled": true,
    "expression": "(kube_horizontalpodautoscaler_status_desired_replicas{job=\"kube-state-metrics\"}  !=kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\"})  and(kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\"}  >kube_horizontalpodautoscaler_spec_min_replicas{job=\"kube-state-metrics\"})  and(kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\"}  <kube_horizontalpodautoscaler_spec_max_replicas{job=\"kube-state-metrics\"})  and changes(kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\"}[15m]) == 0",
    "severity": 1,
    "severity_label": "error",
    "for": "PT15M",
    "annotations": {
      "summary": "Horizontal Pod Autoscaler for {{ $labels.statefulset }} not matched the desired number of replicas.",
      "description": "Horizontal Pod Autoscaler in {{ $labels.cluster}} has not matched the desired number of replicas for longer than 15 minutes. For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT15M"
    }
  },
  {
    "alert": "KubeHpaMaxedOut",
    "enabled": true,
    "expression": "kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\"}  ==kube_horizontalpodautoscaler_spec_max_replicas{job=\"kube-state-metrics\"}",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT15M",
    "annotations": {
      "summary": "Horizontal Pod Autoscaler for {{ $labels.statefulset }} reach the maximum scaling.",
      "description": "Horizontal Pod Autoscaler in {{ $labels.cluster}} has been running at max replicas for longer than 15 minutes. For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT15M"
    }
  },
  {
    "alert": "KubePodCrashLooping",
    "enabled": true,
    "expression": "max_over_time(kube_pod_container_status_waiting_reason{reason=\"CrashLoopBackOff\", job=\"kube-state-metrics\"}[5m]) >= 1",
    "severity": 1,
    "severity_label": "error",
    "for": "PT15M",
    "annotations": {
      "summary": "{{ $labels.namespace }}/{{ $labels.pod }} crash looping in {{ printf \"%.2f\" $value }} / second.",
      "description": "{{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) in {{ $labels.cluster}} is restarting {{ printf \"%.2f\" $value }} / second. For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeJobStale",
    "enabled": true,
    "expression": "sum by(namespace,cluster)(kube_job_spec_completions{job=\"kube-state-metrics\"}) - sum by(namespace,cluster)(kube_job_status_succeeded{job=\"kube-state-metrics\"})  > 0 ",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT360M",
    "annotations": {
      "summary": "Number of stale jobs older than six hours is greater than 0.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT15M"
    }
  },
  {
    "alert": "KubePodContainerRestart",
    "enabled": true,
    "expression": "sum by (namespace, controller, container, cluster)(increase(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\"}[1h])* on(namespace, pod, cluster) group_left(controller) label_replace(kube_pod_owner, \"controller\", \"$1\", \"owner_name\", \"(.*)\")) > 2",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT15M",
    "annotations": {
      "summary": "Pod container restarted more of 2 times in last 1 hour.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubePodReadyStateLow",
    "enabled": true,
    "expression": "sum by (cluster,namespace,deployment)(kube_deployment_status_replicas_ready) / sum by (cluster,namespace,deployment)(kube_deployment_spec_replicas) <.8 or sum by (cluster,namespace,deployment)(kube_daemonset_status_number_ready) / sum by (cluster,namespace,deployment)(kube_daemonset_status_desired_number_scheduled) <.8 ",
    "severity": 1,
    "severity_label": "error",
    "for": "PT5M",
    "annotations": {
      "summary": "Ready state of pods is less than 80%.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT15M"
    }
  },
  {
    "alert": "KubePodFailedState",
    "enabled": true,
    "expression": "sum by (cluster, namespace, controller) (kube_pod_status_phase{phase=\"failed\"} * on(namespace, pod, cluster) group_left(controller) label_replace(kube_pod_owner, \"controller\", \"$1\", \"owner_name\", \"(.*)\"))  > 0",
    "severity": 1,
    "severity_label": "error",
    "for": "PT5M",
    "annotations": {
      "summary": "Number of pods in failed state are greater than 0.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT15M"
    }
  },
  {
    "alert": "KubePodNotReadyByController",
    "enabled": true,
    "expression": "sum by (namespace, controller, cluster) (max by(namespace, pod, cluster) (kube_pod_status_phase{job=\"kube-state-metrics\", phase=~\"Pending|Unknown\"}  ) * on(namespace, pod, cluster) group_left(controller)label_replace(kube_pod_owner,\"controller\",\"$1\",\"owner_name\",\"(.*)\")) > 0",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT15M",
    "annotations": {
      "summary": "{{ $labels.namespace }}/{{ $labels.pod }} in {{ $labels.cluster}} by controller is not ready.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeStatefulSetGenerationMismatch",
    "enabled": true,
    "expression": "kube_statefulset_status_observed_generation{job=\"kube-state-metrics\"} != kube_statefulset_metadata_generation{job=\"kube-state-metrics\"}",
    "severity": 1,
    "severity_label": "error",
    "for": "PT15M",
    "annotations": {
      "summary": "StatefulSet has failed but has not been rolled back.",
      "description": "StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back. For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeJobFailed",
    "enabled": true,
    "expression": "kube_job_failed{job=\"kube-state-metrics\"}  > 0",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT15M",
    "annotations": {
      "summary": "Job {{ $labels.namespace }}/{{ $labels.job_name }} in {{ $labels.cluster}} failed to complete.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeContainerAverageCPUHigh",
    "enabled": true,
    "expression": "sum (rate(container_cpu_usage_seconds_total{image!=\"\", container!=\"POD\"}[5m])) by (pod,cluster,container,namespace) / sum(container_spec_cpu_quota{image!=\"\", container!=\"POD\"}/container_spec_cpu_period{image!=\"\", container!=\"POD\"}) by (pod,cluster,container,namespace) > .95",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT5M",
    "annotations": {
      "summary": "Average CPU usage per container is greater than 95%.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT15M"
    }
  },
  {
    "alert": "KubeContainerAverageMemoryHigh",
    "enabled": true,
    "expression": "avg by (namespace, controller, container, cluster)(((container_memory_working_set_bytes{container!=\"\", image!=\"\", container!=\"POD\"} / on(namespace,cluster,pod,container) group_left kube_pod_container_resource_limits{resource=\"memory\", node!=\"\"})*on(namespace, pod, cluster) group_left(controller) label_replace(kube_pod_owner, \"controller\", \"$1\", \"owner_name\", \"(.*)\")) > .95)",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT10M",
    "annotations": {
      "summary": "Average Memory usage per container is greater than 95%.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  },
  {
    "alert": "KubeletPodStartUpLatencyHigh",
    "enabled": true,
    "expression": "histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job=\"kubelet\"}[5m])) by (cluster, instance, le)) * on(cluster, instance) group_left(node) kubelet_node_name{job=\"kubelet\"} > 60",
    "severity": 2,
    "severity_label": "warning",
    "for": "PT10M",
    "annotations": {
      "summary": "Kubelet Pod startup latency is too high.",
      "description": "For more information on this alert, please refer to this [link](https://aka.ms/aks-alerts/pod-level-recommended-alerts)."
    },
    "alert_resolution": {
      "auto_resolved": true,
      "time_to_resolve": "PT10M"
    }
  }
]
